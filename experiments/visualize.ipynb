{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Purpose Visualization Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Uncertainty Quantifications\n",
    "Assumes `run_experiments` has generated results in the `[EXP_FOLDER]/results` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "EXP_FOLDER = 'MNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(EXP_FOLDER))\n",
    "import config # imported from EXP_FOLDER\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "import matplotlib \n",
    "matplotlib.rc('xtick', labelsize=8) \n",
    "matplotlib.rc('ytick', labelsize=8) \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "from nn_ood.utils.viz import plot_histogram, plot_scatter, plot_rocs_by_error, plot_times, summarize_ood_results, summarize_ood_results_by_error, plot_perf_vs_runtime, plot_rocs_by_dist, generate_table_block\n",
    "from nn_ood.utils.viz import plot_transform_sweep\n",
    "from nn_ood.utils.inspect import norm_by_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "folder_path = os.path.join(EXP_FOLDER, \"results\",\"*\")\n",
    "for filename in glob.glob(folder_path):\n",
    "    name = os.path.basename(filename)\n",
    "    results_dict[name] = torch.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model\n",
    "print(\"Loading models\")\n",
    "models = []\n",
    "for i in range(config.N_MODELS):\n",
    "    print(\"loading model %d\" % i)\n",
    "    filename = os.path.join(EXP_FOLDER, 'models', config.FILENAME + \"_%d\" % i)\n",
    "    state_dict = torch.load(filename)\n",
    "    model = config.make_model()\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(config.device)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "    plt.show()\n",
    "\n",
    "model = models[0]\n",
    "\n",
    "## Ask which uncertainty model to use for viz\n",
    "import ipywidgets as widgets\n",
    "names = list(config.test_unc_models.keys())\n",
    "name_widget = widgets.Dropdown(\n",
    "    options=names,\n",
    "    value=names[0],\n",
    "    description='Uncertainty Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "name_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load unc model\n",
    "name = name_widget.value\n",
    "info = config.test_unc_models[name]   \n",
    "print(info)\n",
    "\n",
    "config.unfreeze_model(model)\n",
    "if 'freeze' in info:\n",
    "    if type(info['freeze']) is bool:\n",
    "        freeze_frac = None\n",
    "    else:\n",
    "        freeze_frac = info['freeze']\n",
    "    config.freeze_model(model, freeze_frac=freeze_frac)     \n",
    "\n",
    "if 'apply_fn' is info:\n",
    "    model.apply(info['apply_fn'])\n",
    "\n",
    "if 'multi_model' in info:\n",
    "    unc_model = info['class'](models, config.dist_fam, info['kwargs'])\n",
    "else:\n",
    "    unc_model = info['class'](model, config.dist_fam, info['kwargs'])\n",
    "\n",
    "if info['load_name'] is not None: \n",
    "    filename = os.path.join(EXP_FOLDER, \"models\", info['load_name']+\"_\"+config.FILENAME)\n",
    "    unc_model.load_state_dict(torch.load(filename), strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Samples from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.viz_datasets(idx=5, unc_model=unc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.viz_transforms(idx=5, unc_model=unc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate plots from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, info in config.plots_to_generate.items():\n",
    "    summarized_results = info['summary_fn'](results_dict, *info['summary_fn_args'], **info['summary_fn_kwargs'])\n",
    "    info['plot_fn'](summarized_results, *info['plot_fn_args'], **info['plot_fn_kwargs'])\n",
    "    if 'legend' in info:\n",
    "        plt.legend(**info['legend'])\n",
    "    if 'title' in info:\n",
    "        plt.title(info['title'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(EXP_FOLDER,filename))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other visualization tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms by technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_techniques = len(config.keys_to_compare)\n",
    "max_cols = 4\n",
    "n_rows = int(np.ceil(n_techniques/max_cols))\n",
    "n_cols = min(max_cols, n_techniques)\n",
    "fig, axes = plt.subplots(n_rows,n_cols,figsize=[5*n_cols, 5*n_rows])\n",
    "axes = axes.flatten()\n",
    "for i, name in enumerate(config.keys_to_compare):\n",
    "    if name not in results_dict:\n",
    "        continue\n",
    "    results = results_dict[name]\n",
    "    \n",
    "    plot_histogram(axes[i], results)\n",
    "    axes[i].set_title(name)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots of Uncertainty vs Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_techniques = len(config.keys_to_compare)\n",
    "n_datasets = len(config.test_dataset_args)\n",
    "fig, axes = plt.subplots(n_techniques,n_datasets,figsize=[2*n_datasets, 2*n_techniques], sharey='row')\n",
    "for i, name in enumerate(config.keys_to_compare):\n",
    "    if name not in results_dict:\n",
    "        continue\n",
    "    results = results_dict[name]\n",
    "    plot_scatter(axes[i], results)\n",
    "    axes[i][n_datasets//2 + n_datasets%2 - 1].set_title(name)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare OoD performance vs Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize_ood_results(results_dict, config.in_dist_splits, config.out_dist_splits,\n",
    "                                keys_to_compare = config.keys_to_compare)\n",
    "\n",
    "plot_perf_vs_runtime(summary,\n",
    "                  colors=config.colors, figsize=[4,2.5], dpi=150, normalize_x=True)\n",
    "plt.title(EXP_FOLDER)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(x=(0.0,config.err_thresh*5, config.err_thresh/10.))\n",
    "def g(x=config.err_thresh):\n",
    "    summary = summarize_ood_results_by_error(results_dict, config.splits_to_use, x,\n",
    "                                keys_to_compare = config.keys_to_compare)\n",
    "\n",
    "    \n",
    "    plot_perf_vs_runtime(summary,\n",
    "                  colors=config.colors, figsize=[4,2.5], dpi=150, normalize_x=True)\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"transforms\" not in dir(config):\n",
    "    raise NameError(\"No transforms to visualize for this experiment\")\n",
    "\n",
    "transforms_results_dict = {}\n",
    "folder_path = os.path.join(EXP_FOLDER, \"results_transforms\",\"*\")\n",
    "for filename in glob.glob(folder_path):\n",
    "    name = os.path.basename(filename)\n",
    "    transforms_results_dict[name] = torch.load(filename)\n",
    "    \n",
    "plot_transform_sweep(transforms_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Runtime Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_times(results_dict, keys_to_compare=config.keys_to_compare, colors=config.colors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
